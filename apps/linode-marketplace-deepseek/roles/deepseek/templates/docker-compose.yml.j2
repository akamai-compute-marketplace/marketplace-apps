services:

  #--------------------------------------------
  # vLLM â€” DeepSeek R1 Distill Qwen
  #--------------------------------------------
  vllm:
    image: vllm/vllm-openai:v0.14.0
    container_name: vllm
    restart: unless-stopped
    shm_size: 8g
    ports:
      - "127.0.0.1:8000:8000"
    volumes:
      - vllm_data:/root/.cache/huggingface
    environment:
      - HF_HOME=/root/.cache/huggingface
      - PYTORCH_ALLOC_CONF=expandable_segments:True
    command:
      - --model=deepseek-ai/DeepSeek-R1-Distill-Qwen-{{ deepseek_model_size }}
      - --max-model-len=16384
      - --gpu-memory-utilization=0.90
      - --tensor-parallel-size={{ tensor_parallel_size }}
      - --max-num-seqs=64
    healthcheck:
      test: ["CMD", "curl", "-f", "-s", "http://localhost:8000/v1/models"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  #--------------------------------------------
  # Open-WebUI
  #--------------------------------------------
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "127.0.0.1:3000:8080"
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      - OPENAI_API_BASE_URL=http://vllm:8000/v1
      - OPENAI_API_KEY=none
      - ANONYMIZED_TELEMETRY=false

volumes:
  vllm_data:
  open_webui_data:
